{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmxh6QTFLuc3eSWvezItJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanil-03/Machine-Learning/blob/main/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM (Support vector machine)\n",
        "It is a supervied ml algorithm that is used for both classification and regression tasks.\n",
        "\n",
        "It is particularly effective in dealing with complex and high dimensional dataset.\n",
        "\n",
        "The fumdamental principle of SVM is to find an optimal hyperplane that separates the data points of different classes.\n"
      ],
      "metadata": {
        "id": "vI_y6xoMmkMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HOW SVM WORKS?\n",
        "\n",
        "We will prepare our dataset.\n",
        "\n",
        "SVM requires labelled training data consisiting of input features and its corresponding class labels.\n",
        "\n",
        "Each data point is represented as a feature vector where each feature desribes a particular characterstics of the data point.\n",
        "\n",
        "The data points should be preprocessed and scaled to ensure that all features have similar scales typically between 0 and 1."
      ],
      "metadata": {
        "id": "jsDHNfg3mu-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYPERPLANE AND MARGIN:-\n",
        "\n",
        "SVM aims to find a hyperplane that best seperates gthe different classes in the feature space.\n",
        "\n",
        "In a binary classification problem the hyperplane is a line in a 2D space or in a higher dimensional space.\n",
        "\n",
        "SVM seeks to maximize the margin(which is the distance between the hyperplane and the nearest data-points from each class) between the two classes.\n",
        "\n",
        "The points on the margin are known as support vectors and play a crucial role in defining the decision boundary."
      ],
      "metadata": {
        "id": "cCYcSDCcm1VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LINEAR SVM:-\n",
        "\n",
        "It finds a linear hyperplane that seperates the classes.\n",
        "\n",
        "The goal is to find the hyperplane that maximizes the margin while minimizing the misclassification of training examples.\n",
        "\n",
        "Mathematically this can be formulated as an optimization problem with the objective of minimizing the weights of the hyperplane subject to the constraint that all training examples lie on the correct side of the hyperplane.\n"
      ],
      "metadata": {
        "id": "iNyp764snA9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NON-LINEAR SVM:-\n",
        "\n",
        "In cases where data is not linearly seperable SVM uses a technique called the kernal trick.\n",
        "\n",
        "The kernal trick maps the orignal input space into a higher dimensional feature space where the datapoints can be linearly seperable.\n",
        "\n",
        "The choice of the kernal depends on the characterstics of the data and the problem at hand.\n"
      ],
      "metadata": {
        "id": "gPy-a4XVnHpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING OF SVM:-\n",
        "\n",
        "SVM training involves finding the optimal hyperplane or decision boundary that seperates the classes.\n",
        "\n",
        "The optimiztion problem is typically solved using methods such as quadratic programming or Sequential minimal optimization.\n",
        "\n",
        "The process involves solving for the weights and bias of the hyperplane that maximize the margin between the classes which defines the decision boundary.\n",
        "\n",
        "The objective os to minimize the regulariztion term while ensuring that the taining examples are correctly classified."
      ],
      "metadata": {
        "id": "36dDK_O1nOJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDICTION OF SVM:-\n",
        "\n",
        "Once the SVM model is trained it can be used to predict the class labels of new unseen data points.\n",
        "\n",
        "This algoritm computes the distance from the test point to the decision boundary.\n",
        "\n",
        "The predicted class label is determined based on which side of the decision boundary the point lies.\n",
        "\n",
        "The decision function can also provide a confidence score or probability estimatefor the prediction."
      ],
      "metadata": {
        "id": "WPaHK0-fnSp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic classification data\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, random_state=42)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear kernal\n",
        "linear_svc = svm.SVC(kernel='linear')\n",
        "linear_svc.fit(X_train, y_train)\n",
        "linear_predictions = linear_svc.predict(X_test)\n",
        "linear_accuracy = accuracy_score(y_test, linear_predictions)\n",
        "print(\"Linear kernal Accuracy:\", linear_accuracy)\n",
        "\n",
        "# Polynomial kernal\n",
        "poly_svc = svm.SVC(kernel='poly', degree=3)\n",
        "poly_svc.fit(X_train, y_train)\n",
        "poly_predictions = poly_svc.predict(X_test)\n",
        "poly_accuracy = accuracy_score(y_test, poly_predictions)\n",
        "print(\"Polynomial kernal Accuracy:\", poly_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRFoTpMEld9u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9099ed31-2279-4bd1-c1b3-e61bab71f6e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear kernal Accuracy: 0.95\n",
            "Polynomial kernal Accuracy: 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IXP34ElHjkGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}