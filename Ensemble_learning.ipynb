{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1SWPgvlV7s1dHala52p5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanil-03/Machine-Learning/blob/main/Ensemble_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning\n",
        "Ensemble learning is a machine learning technique that involves combining multiple models known as base learners to make predictions and decisions.\n",
        "\n",
        "The main idea behind ensemble learning is that by combining the predictions of multiple models.\n",
        "\n",
        "The overall performance can be improved compared to using a single model."
      ],
      "metadata": {
        "id": "zFZIiJ5nZrVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Techniques : -**\n",
        "\n",
        "1. `Bagging : -`\n",
        "\n",
        "    It stands for bootstrap aggregating. It involves creating multiple subsets of the training data through bootstrapping.\n",
        "    \n",
        "    Training a separte base learner of each sucset and combining their predictions. The most common example of bagging is RandomForest Algorithm which combines multiple decision tree.\n",
        "2. `Boosting : -`\n",
        "\n",
        "    It is an iterative ensemble method that focus on training weak learners sequentially and giving more importance to the instances that were misclassified by previous learners.\n",
        "\n",
        "    In this, each bas learner is trained to correct the mistakes made by the previous learners.\n"
      ],
      "metadata": {
        "id": "HXErI5yRasxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advantage of ensemble learning:-\n",
        "\n",
        "1. `Improved performance`: - It can often achieve better performance than individual models especially when the base learners are diverse and complimentry.\n",
        "2. `Robustness`:- It can be more robust to noisy data and outliers because errors made by individual models can be compunsated by others.\n",
        "3. `Reducing overfitting`: - It can help reduce overfitting as the combination of multiple models reduces the risk of relying too heavily on a single model.\n"
      ],
      "metadata": {
        "id": "qMHx-KHrdOjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1lFrhKwaX_RB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate a synthetic dataset\n",
        "x, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initailize the dataset to training and testing sets\n",
        "base_learners = []\n",
        "\n",
        "# Number of base learners\n",
        "num_base_learners = 10\n",
        "\n",
        "# Train the base learners\n",
        "for i in range(num_base_learners):\n",
        "    # create a bootstrap sample of the training data\n",
        "    bootstrap_indices = np.random.choice(len(x_train), size=len(x_train), replace=True)\n",
        "    x_bootstrap = x_train[bootstrap_indices]\n",
        "    y_bootstrap = y_train[bootstrap_indices]\n",
        "\n",
        "    #Create and train a base learner (Random Forest)\n",
        "    base_learner = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "    base_learner.fit(x_bootstrap, y_bootstrap)\n",
        "\n",
        "    # Add the base learner to the list\n",
        "    base_learners.append(base_learner)\n",
        "\n",
        "# make predictions with each base learner\n",
        "base_predictions = []\n",
        "for base_learner in base_learners:\n",
        "    y_pred = base_learner.predict(x_test)\n",
        "    base_predictions.append(y_pred)\n",
        "\n",
        "#combine the predictions using majority voting\n",
        "ensemble_predictions = np.round(np.mean(base_predictions, axis=0))\n",
        "\n",
        "#calculate the accuracy of the ensemble predictions\n",
        "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
        "print(\"Ensemble Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sJmlgBBgy14",
        "outputId": "0e954d82-e12e-4fb7-931c-d7055a630742"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cO1a0UCpiiyp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}